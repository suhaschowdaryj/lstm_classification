{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LSTM_multiclass_text_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFBg2y5GZOCE"
      },
      "source": [
        "# LSTM in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD5MKSQjZOCG"
      },
      "source": [
        "#library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "#import jovian\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASXSGJWaqtd"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import collections\n",
        "import timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOUoUAANtnFq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpMuVcUrZOCM"
      },
      "source": [
        "## Basic LSTM in Pytorch with random numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxKMy3aFZOCv"
      },
      "source": [
        "## Multiclass Text Classification\n",
        "\n",
        "We are going to predict item ratings based on customer reviews bsed on this dataset from Kaggle:\n",
        "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Ofk7qQacaf"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhqjw3F9ajNa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "38028af1-4aad-44e2-b75e-4846bcf6935d"
      },
      "source": [
        "mypath = 'data'\n",
        "folders = [f for f in os.listdir(mypath)]\n",
        "folder_indexes = {v:i for i,v in enumerate(folders)}\n",
        "folder_indexes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alt.atheism': 10,\n",
              " 'comp.graphics': 3,\n",
              " 'comp.os.ms-windows.misc': 12,\n",
              " 'comp.sys.ibm.pc.hardware': 9,\n",
              " 'comp.sys.mac.hardware': 16,\n",
              " 'comp.windows.x': 17,\n",
              " 'misc.forsale': 13,\n",
              " 'rec.autos': 8,\n",
              " 'rec.motorcycles': 7,\n",
              " 'rec.sport.baseball': 4,\n",
              " 'rec.sport.hockey': 18,\n",
              " 'sci.crypt': 19,\n",
              " 'sci.electronics': 1,\n",
              " 'sci.med': 6,\n",
              " 'sci.space': 5,\n",
              " 'soc.religion.christian': 15,\n",
              " 'talk.politics.guns': 0,\n",
              " 'talk.politics.mideast': 2,\n",
              " 'talk.politics.misc': 14,\n",
              " 'talk.religion.misc': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKc1-nHVawxn"
      },
      "source": [
        "classes = []\n",
        "X = []\n",
        "y = []\n",
        "mypath = 'data'\n",
        "folders = [f for f in os.listdir(mypath)]\n",
        "\n",
        "for folder in folders:\n",
        "    cls = []\n",
        "    f_path = os.path.join(mypath,folder)\n",
        "    for files in os.listdir(f_path):\n",
        "        text = ''\n",
        "        path = os.path.join(f_path,files)\n",
        "        with open(path,'r',errors='ignore',encoding=\"utf8\") as f:\n",
        "            cur = f.read()\n",
        "            cur = cur.lower()\n",
        "            cur = re.sub(r'[\\w\\.-]+@[\\w\\.-]+',' ',cur)\n",
        "            cur = re.sub(\"[^a-zA-Z,.']\", '  ', cur)\n",
        "            cur = re.sub(r'\\.{2,}',' ',cur)   \n",
        "            cur = re.sub('\\s+',' ',cur)\n",
        "            X.append(cur)\n",
        "        y.append(int(folder_indexes[folder]))\n",
        "        #cls.append(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao1AAfa1rrW-"
      },
      "source": [
        "df = pd.DataFrame({'doc':X,\n",
        "                         'labels':y})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-25oJEOtZODF"
      },
      "source": [
        "#tokenization\n",
        "tok = spacy.load('en')\n",
        "def tokenize (text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
        "    nopunct = regex.sub(\" \", text.lower())\n",
        "    return [token.text for token in tok.tokenizer(nopunct)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn3ChIJMr5CP"
      },
      "source": [
        "#count number of occurences of each word\n",
        "counts = Counter()\n",
        "for index, row in df.iterrows():\n",
        "    counts.update(tokenize(row['doc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc_J11SWsMxL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "73c3a5c9-1aea-496b-ed7a-7fd773427096"
      },
      "source": [
        "#deleting infrequent words\n",
        "print(\"num_words before:\",len(counts.keys()))\n",
        "for word in list(counts):\n",
        "    if counts[word] < 2:\n",
        "        del counts[word]\n",
        "print(\"num_words after:\",len(counts.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_words before: 85667\n",
            "num_words after: 55497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idRosZ_OZODT"
      },
      "source": [
        "#creating vocabulary\n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXpgbGjSZODa"
      },
      "source": [
        "def encode_sentence(text, vocab2index, N=500):\n",
        "    tokenized = tokenize(text)\n",
        "    encoded = np.zeros(N, dtype=int)\n",
        "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "    length = min(N, len(enc1))\n",
        "    encoded[:length] = enc1[:length]\n",
        "    return encoded, length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CwIMHfHZODg"
      },
      "source": [
        "df['encoded'] = df['doc'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxnEZ-mqtHlM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3252f6b7-97d7-43f0-ade7-b16dd149ac79"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>labels</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a few comments on the atf's botched handling o...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in article qvh n , gedaliah friedenberg writes...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[19, 295, 296, 297, 14, 298, 299, 300, 19, 29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.acns.colostate.edu douglas craig holland writ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[40, 547, 1, 548, 549, 550, 551, 300, 552, 40...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>has anyone noticed or commented on the fact th...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[106, 668, 657, 249, 669, 5, 6, 670, 31, 671,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in article r ito c. d. tavares writes it's har...</td>\n",
              "      <td>0</td>\n",
              "      <td>[[19, 295, 553, 702, 652, 40, 56, 40, 703, 300...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  ...                                            encoded\n",
              "0  a few comments on the atf's botched handling o...  ...  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...\n",
              "1  in article qvh n , gedaliah friedenberg writes...  ...  [[19, 295, 296, 297, 14, 298, 299, 300, 19, 29...\n",
              "2  .acns.colostate.edu douglas craig holland writ...  ...  [[40, 547, 1, 548, 549, 550, 551, 300, 552, 40...\n",
              "3  has anyone noticed or commented on the fact th...  ...  [[106, 668, 657, 249, 669, 5, 6, 670, 31, 671,...\n",
              "4  in article r ito c. d. tavares writes it's har...  ...  [[19, 295, 553, 702, 652, 40, 56, 40, 703, 300...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U94xjeIXZODj"
      },
      "source": [
        "#check how balanced the dataset is\n",
        "Counter(df['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VznVUK7ZODm"
      },
      "source": [
        "X = list(df['encoded'])\n",
        "y = list(df['labels'])\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMSQY68aZODq"
      },
      "source": [
        "#### Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFXBVh_YZODq"
      },
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv4BP1LiZODt"
      },
      "source": [
        "train_ds = ReviewsDataset(X_train, y_train)\n",
        "valid_ds = ReviewsDataset(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj95-qx1ZOD1"
      },
      "source": [
        "batch_size = 5000\n",
        "vocab_size = len(words)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGXpYLXhy80C"
      },
      "source": [
        "def train_model(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.long()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
        "\n",
        "def validation_metrics (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    sum_rmse = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        y = y.long()\n",
        "        y_hat = model(x, l)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
        "    return sum_loss/total, correct/total, sum_rmse/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCa0tHHCZOD5"
      },
      "source": [
        "### LSTM with fixed length input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UweA8PuZOD6"
      },
      "source": [
        "class LSTM_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 20)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ3UncVgZOD_"
      },
      "source": [
        "model_fixed =  LSTM_fixed_len(vocab_size, 50, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1V9DsfcZOEC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "76797385-9e6c-436f-f6ef-3cd3b3e46bf5"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 3.012, val loss 3.004, val accuracy 0.052, and val rmse 7.430\n",
            "train loss 2.982, val loss 2.985, val accuracy 0.058, and val rmse 6.500\n",
            "train loss 2.963, val loss 2.974, val accuracy 0.062, and val rmse 6.070\n",
            "train loss 2.940, val loss 2.969, val accuracy 0.061, and val rmse 6.106\n",
            "train loss 2.916, val loss 2.971, val accuracy 0.062, and val rmse 6.088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2f908333c01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-b87678a43f6a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-eee0f69356fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, l)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpRySRBBZOEF"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RC80ThNZOEJ"
      },
      "source": [
        "train_model(model_fixed, epochs=30, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCO-KkryZOEM"
      },
      "source": [
        "### LSTM with variable length input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKO3YTSsZOEM"
      },
      "source": [
        "class LSTM_variable_input(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        \n",
        "    def forward(self, x, s):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
        "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
        "        out = self.linear(ht[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc1Nh81iZOER"
      },
      "source": [
        "model = LSTM_variable_input(vocab_size, 50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TgNtb5xZOEV",
        "outputId": "ac7a8f4e-19c0-475f-8c7a-13570855b810"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.328, val loss 1.250, val accuracy 0.515, and val rmse 1.312\n",
            "train loss 1.031, val loss 1.063, val accuracy 0.577, and val rmse 1.017\n",
            "train loss 0.904, val loss 0.995, val accuracy 0.603, and val rmse 0.941\n",
            "train loss 0.849, val loss 1.000, val accuracy 0.599, and val rmse 0.940\n",
            "train loss 0.845, val loss 1.009, val accuracy 0.598, and val rmse 0.921\n",
            "train loss 0.834, val loss 1.005, val accuracy 0.593, and val rmse 0.902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3NY99dMZOEb",
        "outputId": "2fcc7fca-6188-4bef-929f-42e4ffca2dee"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.828, val loss 1.000, val accuracy 0.599, and val rmse 0.920\n",
            "train loss 0.790, val loss 0.989, val accuracy 0.605, and val rmse 0.894\n",
            "train loss 0.775, val loss 0.992, val accuracy 0.614, and val rmse 0.884\n",
            "train loss 0.755, val loss 0.994, val accuracy 0.597, and val rmse 0.883\n",
            "train loss 0.738, val loss 0.987, val accuracy 0.608, and val rmse 0.872\n",
            "train loss 0.741, val loss 1.005, val accuracy 0.611, and val rmse 0.888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAtkzmKZOEj",
        "outputId": "61bb9e20-db8f-4db4-ad3f-8fe419784940"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 0.758, val loss 1.028, val accuracy 0.616, and val rmse 0.884\n",
            "train loss 0.725, val loss 0.994, val accuracy 0.621, and val rmse 0.877\n",
            "train loss 0.715, val loss 0.999, val accuracy 0.607, and val rmse 0.881\n",
            "train loss 0.707, val loss 1.008, val accuracy 0.608, and val rmse 0.879\n",
            "train loss 0.698, val loss 1.018, val accuracy 0.615, and val rmse 0.890\n",
            "train loss 0.686, val loss 1.017, val accuracy 0.603, and val rmse 0.893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d7WOckSZOEn"
      },
      "source": [
        "### LSTM with pretrained Glove word embeddings\n",
        "\n",
        "Download weights from : https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BfStMYiZOEo"
      },
      "source": [
        "def load_glove_vectors(glove_file=\"./data/glove.6B/glove.6B.50d.txt\"):\n",
        "    \"\"\"Load the glove word vectors\"\"\"\n",
        "    word_vectors = {}\n",
        "    with open(glove_file) as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
        "    return word_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6K5fWj9ZOEs"
      },
      "source": [
        "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
        "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "    vocab_size = len(word_counts) + 2\n",
        "    vocab_to_idx = {}\n",
        "    vocab = [\"\", \"UNK\"]\n",
        "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
        "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
        "    vocab_to_idx[\"UNK\"] = 1\n",
        "    i = 2\n",
        "    for word in word_counts:\n",
        "        if word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "        vocab_to_idx[word] = i\n",
        "        vocab.append(word)\n",
        "        i += 1   \n",
        "    return W, np.array(vocab), vocab_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hExBga35ZOEw"
      },
      "source": [
        "word_vecs = load_glove_vectors()\n",
        "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvbEinf2ZOEz"
      },
      "source": [
        "class LSTM_glove_vecs(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 5)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0bUIB4ZOE5"
      },
      "source": [
        "model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e60npxrAZOFA",
        "outputId": "df908cd4-3b50-42bc-8bd1-6e140c9c7542"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.281, val loss 1.255, val accuracy 0.556, and val rmse 1.355\n",
            "train loss 1.210, val loss 1.207, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.206, val loss 1.204, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.201, val loss 1.202, val accuracy 0.556, and val rmse 1.354\n",
            "train loss 1.173, val loss 1.168, val accuracy 0.557, and val rmse 1.352\n",
            "train loss 1.131, val loss 1.122, val accuracy 0.562, and val rmse 1.249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxiRe1efZOFD",
        "outputId": "255b3d7d-1118-4a05-b3b5-dc79b77f16dc"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.112, val loss 1.113, val accuracy 0.556, and val rmse 1.349\n",
            "train loss 1.061, val loss 1.051, val accuracy 0.570, and val rmse 1.109\n",
            "train loss 1.014, val loss 1.014, val accuracy 0.582, and val rmse 1.058\n",
            "train loss 0.979, val loss 0.990, val accuracy 0.599, and val rmse 0.995\n",
            "train loss 0.948, val loss 0.961, val accuracy 0.610, and val rmse 0.950\n",
            "train loss 0.923, val loss 0.952, val accuracy 0.612, and val rmse 0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz-AToriZOFI",
        "outputId": "86c9d2e0-a623-4947-8fb6-a2ff4c897c74"
      },
      "source": [
        "train_model(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss 1.189, val loss 1.014, val accuracy 0.586, and val rmse 1.033\n",
            "train loss 0.946, val loss 0.964, val accuracy 0.606, and val rmse 0.950\n",
            "train loss 0.912, val loss 0.951, val accuracy 0.612, and val rmse 0.941\n",
            "train loss 0.895, val loss 0.949, val accuracy 0.615, and val rmse 0.913\n",
            "train loss 0.886, val loss 0.947, val accuracy 0.617, and val rmse 0.901\n",
            "train loss 0.872, val loss 0.938, val accuracy 0.621, and val rmse 0.890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfT_XL3ZOFL"
      },
      "source": [
        "## Predicting ratings using regression instead of classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAGbUb-qZOFM"
      },
      "source": [
        "def train_model_regr(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.float()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss = validation_metrics_regr(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train mse %.3f val rmse %.3f\" % (sum_loss/total, val_loss))\n",
        "\n",
        "def validation_metrics_regr (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        y = y.float()\n",
        "        y_hat = model(x, l)\n",
        "        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "    return sum_loss/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTHgSaVSZOFQ"
      },
      "source": [
        "class LSTM_regr(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nzeWkxXZOFV"
      },
      "source": [
        "model =  LSTM_regr(vocab_size, 50, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgO2f-YZZOFa",
        "outputId": "6e1a75b4-b2a2-4fda-f9cc-4b04b0e8ee08"
      },
      "source": [
        "train_model_regr(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train mse 1.663 val rmse 1.313\n",
            "train mse 1.215 val rmse 1.125\n",
            "train mse 1.151 val rmse 1.109\n",
            "train mse 1.114 val rmse 1.115\n",
            "train mse 1.082 val rmse 1.121\n",
            "train mse 1.043 val rmse 1.116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JzsAjrHZOFe",
        "outputId": "2550e526-d6c3-4f38-cf44-c7bfb5d5a4eb"
      },
      "source": [
        "train_model_regr(model, epochs=30, lr=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train mse 1.214 val rmse 1.193\n",
            "train mse 0.884 val rmse 1.032\n",
            "train mse 0.631 val rmse 0.903\n",
            "train mse 0.483 val rmse 0.837\n",
            "train mse 0.416 val rmse 0.806\n",
            "train mse 0.363 val rmse 0.799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFaG12gmZOFj",
        "outputId": "80c12e67-3065-4fa7-a6e0-5cfb3f692cb2"
      },
      "source": [
        "jovian.commit(\"lstm multiclass text classification, regression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Saving notebook..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiR9pCjbZOFu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}